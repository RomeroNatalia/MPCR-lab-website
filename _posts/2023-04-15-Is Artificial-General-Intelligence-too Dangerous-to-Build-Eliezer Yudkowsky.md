---
title: "Is Artificial General Intelligence too Dangerous to Build? Eliezer Yudkowsky discusses his rationale for ceasing the development of Als more sophisticated than GPT-4"
image: news-pictures/Eliezer-Yudkowsky.jpg
categories: [Outreach]
tags: [AI, Workshop] # Searchable Terms
include_in_search: false
hidden: false # Hides from News Feed useful for direct links only
featured: true
author: Susan-Schneider
images:
  # - path: /uploads/news-pictures/2023-Image-Segmentation.png
  # - path: /uploads/news-pictures/2023-AI-in-Science-and-Medicine-Example.png
  # - path: /uploads/news-pictures/2023-Spring-Math-of-Data-Science.jpg
---

An [open letter][open-letter] published on March 22, 2023 calls for Al labs to immediately pause for at least 6 months the training of Al systems more powerful than GPT-4. [In response][Yudkowsky_response], Yudkowsky argues that this proposal does not do enough to protect us from the risks of losing control of superintelligent AI. Join us for an interactive Q&A with Yudkowsky about Al Safety! Dr. Mark Bailey of National Intelligence University will moderate the discussion.

Eliezer Yudkowsky is a decision theorist from the United States and leads research at the [Machine Intelligence Research Institute (MIRI)][miri]. He has been working on aligning Artificial General Intelligence since 2001 and is widely regarded as a founder of the field.

Dr. Mark Bailey is the Chair of the Cyber Intelligence and Data Science Department, as well as the Co-Director of the Data Science Intelligence Center, at the National Intelligence University  

# Date / Time / Location
* Wednesday, April 19th, 2023, 4:00 pm - 5:30 pm EST  
* The Rubin and Cindy Gruber Sandbox, Wimberly Library, Boca Raton FL

A video of his talk is available at the Center for the Future Mind YouTube page:
* [https://www.youtube.com/watch?v=3_YX6AgxxYw](https://www.youtube.com/watch?v=3_YX6AgxxYw)

<iframe src="https://www.youtube-nocookie.com/embed/3_YX6AgxxYw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


[open-letter]: https://futureoflife.org/open-letter/pause-giant-ai-experiments/
[Yudkowsky_response]: https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/
[miri]: https://intelligence.org/
[zoom_url]: https://bit.ly/3zRefhR

